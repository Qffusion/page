<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [['$','$'], ['\\(','\\)']],
          processEscapes: true
      }
  });
    </script>
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Qffusion">
  <meta property="og:title" content="Qffusion"/>
  <meta property="og:description" content="Qffusion is a dual-frame-guided framework for portrait video editing."/>
  <meta property="og:url" content="https://qffusion.github.io/page/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/overview_v4.pdf" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">



  <title>Qffusion</title>
  <link rel="icon" type="image/x-icon" href="static/images/q.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Qffusion: Controllable Portrait Video Editing via Quadrant-Grid Attention Learning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=ym_t6QYAAAAJ" target="_blank">Maomao Li<sup>*</sup></a><sup></sup>,</span>
                <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?user=Xf5_TfcAAAAJ&hl=zh-CN" target="_blank">Lijian Lin<sup>*</sup></a><sup></sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=B1Z1vTMAAAAJ&hl=zh-CN" target="_blank">Yunfei Liu<sup>$\dagger$</sup></a><sup></sup>,</
                  </span> 
                  <span class="author-block">
                    <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=qhp9rIMAAAAJ" target="_blank">Ye Zhu<sup>$\dagger$</sup></a><sup></sup>,</
                  </span>  
                 <span class="author-block">
                  <a href="https://yu-li.github.io/" target="_blank">Yu Li<sup>$\dagger$</sup></a><sup></sup>,</span>                               
                  </div>
                   <br>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"></sup>International Digital Economy Academy (IDEA)
                    &nbsp;&nbsp;
                    <br>
                     <br>
                     <!-- <h2 class="title">TVCG 2025</h2> -->
                    <br>
                    <!--<br>Conferance name and year</span>-->
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>


    

                  <!-- Github link -->
                 <!--  <span class="link-block">
                    <a href="https://github.com/Qffusion/Qffusion" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                &nbsp;&nbsp; -->
                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2501.06438" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/FZnDn4wBPqY?si=g52FUcutAB7CEQv_" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
      <h2 class="subtitle has-text-centered">
       This paper presents Qffusion, a novel dual-frame-guided framework for portrait video editing. Specifically, we first specify editing requirements by modifying the start and end frames with professional software (such as Photoshop). Then, we use Qffusion to propagate the user-given fine-grained local modification (such as age, makeup, hair, style, and wearing sunglasses).
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          This paper presents Qffusion, a dual-frame-guided framework for portrait video editing. Specifically, we consider a design principle of ``animation for editing'', and train Qffusion as a general animation framework from two still reference images while we can use it for portrait video editing easily by applying modified start and end frames as references during inference. Leveraging the powerful generative power of Stable Diffusion, we propose a Quadrant-grid Arrangement (QGA) scheme for latent re-arrangement, which arranges the latent codes of two reference images and that of four facial conditions into a four-grid fashion, separately. Then, we fuse features of these two modalities and use self-attention for both appearance and temporal learning, where representations at different times are jointly modeled under QGA. Our Qffusion can achieve stable video editing without additional networks or complex training stages, where only the input format of Stable Diffusion is modified. Further, we propose a Quadrant-grid Propagation (QGP) inference strategy, which enjoys a unique advantage on stable arbitrary-length video generation by processing reference and condition frames recursively. Through extensive experiments, Qffusion consistently outperforms state-of-the-art techniques on portrait video editing. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
           <div class="item">
        <!-- Your image here -->
        <img src="static/images/overview_v4.png" alt="MY ALT TEXT"/>
        </p> 
          Overview illustration of Qffusion. As for training, we first design a Quadrant-grid Arrangement (QGA) scheme for latent re-arrangement, which arranges the latent codes of two reference images and that of four portrait landmarks into a four-grid fashion, separately.  Then, we fuse features of these two modalities and use self-attention for both appearance and temporal learning. Here, the facial identity features [1] are also put into cross-attention mechanism in the denoising U-Net for further identity constraint. During inference, a stable video is generated via our proposed Quadrant-grid Propagation (QGP) strategy.
        </p>
      </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->





<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Portrait Video Editing</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/3.mp4"
            type="video/mp4">
          </video>
        </div>   
      </div>
          </p> 
          We compare Qffusion with four recent video editing methods: TokenFlow [2], Rerender-A-Video [3], Codef [4] and AnyV2V [5] on portrait video editing. Besides, we provide animation results from ControlNext-SVD [6]. Note that our method requires both modified start and end frames ($\mathbf{I}^s$ and $\mathbf{I}^e$) as editing signals, where $\mathbf{I}^e$ is omitted here for simplicity.
        </p> 
    </div> 
  </div>
</section>
<!-- End video carousel -->




<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @article{qffusion,
        title = {Qffusion: Controllable Portrait Video Editing via Quadrant-Grid Attention Learning},
        author = {Maomao Li, Lijian Lin, Yunfei Liu, Ye Zhu, Yu Li},
        journal={arXiv preprint arXiv:2501.06438},
        year={2025}
        }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->



<!--BibTex citation -->
  <section class="section" id="Reference">
    <div class="container is-max-desktop content">
      <h2 class="title">Reference</h2>
      <!--<pre><code>-->
      <p> [1] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, “Arcface: Additive angular margin loss for deep face recognition,” in CVPR, 2019, pp. 4690–4699.
      <br />
      [2] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing.
      <br />
      [3] S. Yang, Y. Zhou, Z. Liu, and C. C. Loy, “Rerender a video: Zero-shot text-guided video-to-video translation,” in SIGGRAPH Asia 2023 Conference Papers, 2023.
      <br />
      [4] H. Ouyang, Q. Wang, Y. Xiao, Q. Bai, J. Zhang, K. Zheng, X. Zhou, Q. Chen, and Y. Shen, “Codef: Content deformation fields for temporally consistent video processing,” CVPR, 2024.
      <br />
      [5] M. Ku, C. Wei, W. Ren, H. Yang, and W. Chen, “Anyv2v: A plug-and-play framework for any video-to-video editing tasks,” TMLR, 2024.
      <br />
      [6] B. Peng, J. Wang, Y. Zhang, W. Li, M.-C. Yang, and J. Jia, “Controlnext: Powerful and efficient control for image and video generation,” arXiv preprint arXiv:2408.06070, 2024.
      </p> 
      <!-- </code></pre>-->
    </div>
</section>
<!--End BibTex citation -->

      
        
      </div>
    </div>
  </section>
<!--End paper poster -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>